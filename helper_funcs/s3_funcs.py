from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import os

# Define the function to read the file from S3
def read_file_from_s3(**kwargs):
    # Create an S3Hook to interact with Amazon S3
    s3_hook = S3Hook(aws_conn_id=kwargs["s3_conn_id"])

    # Download the file from S3 to the local file system
    try:
        s3_hook.download_file(bucket_name=kwargs["s3_bucket_name"], key=kwargs["s3_nodes_file_name"], local_path=kwargs["local_nodes_file_path"], preserve_file_name=True, use_autogenerated_subdir=False)
    except Exception as e:
        print("Nodes Download Failed")

    try:
        s3_hook.download_file(bucket_name=kwargs["s3_bucket_name"], key=kwargs["s3_edges_file_name"], local_path=kwargs["local_edges_file_path"], preserve_file_name=True, use_autogenerated_subdir=False)
    except Exception as e:
        print("Edges Download Failed")

def load_file_to_s3(**kwargs):
    s3_hook = S3Hook(aws_conn_id=kwargs["s3_conn_id"])

    filename = kwargs["nodes_local_path"] + kwargs["preprocessed_nodes_file_name"]
    s3_hook.load_file(filename=filename, bucket_name=kwargs["s3_bucket_name"], key=kwargs["s3_nodes_file_name"], replace=True)

    filename = kwargs["edges_local_path"] + kwargs["preprocessed_edges_file_name"]
    s3_hook.load_file(filename=filename, bucket_name=kwargs["s3_bucket_name"], key=kwargs["s3_edges_file_name"], replace=True)


    
